{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab65bb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import senteval\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import json\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c2b78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers.readers import InputExample\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import gzip\n",
    "import csv\n",
    "\n",
    "sts_dataset_path = '../datasets/stsbenchmark.tsv.gz'\n",
    "\n",
    "if not os.path.exists(sts_dataset_path):\n",
    "    util.http_get('https://sbert.net/datasets/stsbenchmark.tsv.gz', sts_dataset_path)\n",
    "\n",
    "texts = []\n",
    "with gzip.open(sts_dataset_path, 'rt', encoding='utf8') as fIn:\n",
    "    reader = csv.DictReader(fIn, delimiter='\\t', quoting=csv.QUOTE_NONE)\n",
    "    for row in reader:\n",
    "        if row['split'] == 'test':\n",
    "            texts.append(row['sentence1'])\n",
    "            texts.append(row['sentence2'])\n",
    "\n",
    "class BaseDataset(Dataset):\n",
    "    def __init__(self, texts, scale=None):\n",
    "        # samples in format [[sentences1], [sentences2], [scores or labels]]\n",
    "        # scores range is [0, 5], labels are strings\n",
    "        self.texts = []\n",
    "        for t in texts:\n",
    "            self.texts.append(t)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.texts[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "dataset = BaseDataset(texts)\n",
    "loader = DataLoader(dataset, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dce084d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# taken from https://github.com/google-research/google-research/blob/master/representation_similarity/Demo.ipynb\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def gram_linear(x):\n",
    "    \"\"\"Compute Gram (kernel) matrix for a linear kernel.\n",
    "\n",
    "    Args:\n",
    "    x: A num_examples x num_features matrix of features.\n",
    "\n",
    "    Returns:\n",
    "    A num_examples x num_examples Gram matrix of examples.\n",
    "    \"\"\"\n",
    "    return x.dot(x.T)\n",
    "\n",
    "\n",
    "def gram_rbf(x, threshold=1.0):\n",
    "    \"\"\"Compute Gram (kernel) matrix for an RBF kernel.\n",
    "\n",
    "    Args:\n",
    "    x: A num_examples x num_features matrix of features.\n",
    "    threshold: Fraction of median Euclidean distance to use as RBF kernel\n",
    "        bandwidth. (This is the heuristic we use in the paper. There are other\n",
    "        possible ways to set the bandwidth; we didn't try them.)\n",
    "\n",
    "    Returns:\n",
    "    A num_examples x num_examples Gram matrix of examples.\n",
    "    \"\"\"\n",
    "    dot_products = x.dot(x.T)\n",
    "    sq_norms = np.diag(dot_products)\n",
    "    sq_distances = -2 * dot_products + sq_norms[:, None] + sq_norms[None, :]\n",
    "    sq_median_distance = np.median(sq_distances)\n",
    "    return np.exp(-sq_distances / (2 * threshold ** 2 * sq_median_distance))\n",
    "\n",
    "\n",
    "def center_gram(gram, unbiased=False):\n",
    "    \"\"\"Center a symmetric Gram matrix.\n",
    "\n",
    "    This is equvialent to centering the (possibly infinite-dimensional) features\n",
    "    induced by the kernel before computing the Gram matrix.\n",
    "\n",
    "    Args:\n",
    "    gram: A num_examples x num_examples symmetric matrix.\n",
    "    unbiased: Whether to adjust the Gram matrix in order to compute an unbiased\n",
    "        estimate of HSIC. Note that this estimator may be negative.\n",
    "\n",
    "    Returns:\n",
    "    A symmetric matrix with centered columns and rows.\n",
    "    \"\"\"\n",
    "    if not np.allclose(gram, gram.T):\n",
    "        raise ValueError('Input must be a symmetric matrix.')\n",
    "    gram = gram.copy()\n",
    "\n",
    "    if unbiased:\n",
    "        # This formulation of the U-statistic, from Szekely, G. J., & Rizzo, M.\n",
    "        # L. (2014). Partial distance correlation with methods for dissimilarities.\n",
    "        # The Annals of Statistics, 42(6), 2382-2412, seems to be more numerically\n",
    "        # stable than the alternative from Song et al. (2007).\n",
    "        n = gram.shape[0]\n",
    "        np.fill_diagonal(gram, 0)\n",
    "        means = np.sum(gram, 0, dtype=np.float64) / (n - 2)\n",
    "        means -= np.sum(means) / (2 * (n - 1))\n",
    "        gram -= means[:, None]\n",
    "        gram -= means[None, :]\n",
    "        np.fill_diagonal(gram, 0)\n",
    "    else:\n",
    "        means = np.mean(gram, 0, dtype=np.float64)\n",
    "        means -= np.mean(means) / 2\n",
    "        gram -= means[:, None]\n",
    "        gram -= means[None, :]\n",
    "\n",
    "    return gram\n",
    "\n",
    "\n",
    "def cka(gram_x, gram_y, debiased=False):\n",
    "    \"\"\"Compute CKA.\n",
    "\n",
    "    Args:\n",
    "    gram_x: A num_examples x num_examples Gram matrix.\n",
    "    gram_y: A num_examples x num_examples Gram matrix.\n",
    "    debiased: Use unbiased estimator of HSIC. CKA may still be biased.\n",
    "\n",
    "    Returns:\n",
    "    The value of CKA between X and Y.\n",
    "    \"\"\"\n",
    "    gram_x = center_gram(gram_x, unbiased=debiased)\n",
    "    gram_y = center_gram(gram_y, unbiased=debiased)\n",
    "\n",
    "    # Note: To obtain HSIC, this should be divided by (n-1)**2 (biased variant) or\n",
    "    # n*(n-3) (unbiased variant), but this cancels for CKA.\n",
    "    scaled_hsic = gram_x.ravel().dot(gram_y.ravel())\n",
    "\n",
    "    normalization_x = np.linalg.norm(gram_x)\n",
    "    normalization_y = np.linalg.norm(gram_y)\n",
    "    return scaled_hsic / (normalization_x * normalization_y)\n",
    "\n",
    "\n",
    "def _debiased_dot_product_similarity_helper(xty, sum_squared_rows_x, sum_squared_rows_y, squared_norm_x, squared_norm_y, n):\n",
    "    \"\"\"Helper for computing debiased dot product similarity (i.e. linear HSIC).\"\"\"\n",
    "    # This formula can be derived by manipulating the unbiased estimator from\n",
    "    # Song et al. (2007).\n",
    "    return (xty - n / (n - 2.) * sum_squared_rows_x.dot(sum_squared_rows_y)\n",
    "      + squared_norm_x * squared_norm_y / ((n - 1) * (n - 2)))\n",
    "\n",
    "\n",
    "def feature_space_linear_cka(features_x, features_y, debiased=False):\n",
    "    \"\"\"Compute CKA with a linear kernel, in feature space.\n",
    "\n",
    "    This is typically faster than computing the Gram matrix when there are fewer\n",
    "    features than examples.\n",
    "\n",
    "    Args:\n",
    "    features_x: A num_examples x num_features matrix of features.\n",
    "    features_y: A num_examples x num_features matrix of features.\n",
    "    debiased: Use unbiased estimator of dot product similarity. CKA may still be\n",
    "        biased. Note that this estimator may be negative.\n",
    "\n",
    "    Returns:\n",
    "    The value of CKA between X and Y.\n",
    "    \"\"\"\n",
    "    features_x = features_x - np.mean(features_x, 0, keepdims=True)\n",
    "    features_y = features_y - np.mean(features_y, 0, keepdims=True)\n",
    "\n",
    "    dot_product_similarity = np.linalg.norm(features_x.T.dot(features_y)) ** 2\n",
    "    normalization_x = np.linalg.norm(features_x.T.dot(features_x))\n",
    "    normalization_y = np.linalg.norm(features_y.T.dot(features_y))\n",
    "\n",
    "    if debiased:\n",
    "        n = features_x.shape[0]\n",
    "        # Equivalent to np.sum(features_x ** 2, 1) but avoids an intermediate array.\n",
    "        sum_squared_rows_x = np.einsum('ij,ij->i', features_x, features_x)\n",
    "        sum_squared_rows_y = np.einsum('ij,ij->i', features_y, features_y)\n",
    "        squared_norm_x = np.sum(sum_squared_rows_x)\n",
    "        squared_norm_y = np.sum(sum_squared_rows_y)\n",
    "\n",
    "        dot_product_similarity = _debiased_dot_product_similarity_helper(\n",
    "            dot_product_similarity, sum_squared_rows_x, sum_squared_rows_y,\n",
    "            squared_norm_x, squared_norm_y, n)\n",
    "        normalization_x = np.sqrt(_debiased_dot_product_similarity_helper(\n",
    "            normalization_x ** 2, sum_squared_rows_x, sum_squared_rows_x,\n",
    "            squared_norm_x, squared_norm_x, n))\n",
    "        normalization_y = np.sqrt(_debiased_dot_product_similarity_helper(\n",
    "            normalization_y ** 2, sum_squared_rows_y, sum_squared_rows_y,\n",
    "            squared_norm_y, squared_norm_y, n))\n",
    "\n",
    "    return dot_product_similarity / (normalization_x * normalization_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f980a3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda:0\"\n",
    "\n",
    "def batch_to_device(d, device):\n",
    "    return {k: v.to(device) for k, v in d.items()}\n",
    "    \n",
    "class MeanPooling(nn.Module):\n",
    "    def __init__(self, starting_state):\n",
    "        super().__init__()\n",
    "        self.starting_state = starting_state\n",
    "\n",
    "    def forward(self, x, attention_mask):\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(x.size()).float().to(DEVICE)\n",
    "        emb_sum = torch.sum(x * input_mask_expanded, dim=1)\n",
    "        sum_mask = torch.clamp(input_mask_expanded.sum(dim=1), min=1e-9) # denominator\n",
    "        emb_mean = emb_sum / sum_mask\n",
    "        return emb_mean\n",
    "    \n",
    "class GenericModel:\n",
    "\n",
    "    def __init__(self, model_name, starting_state=12, path=None):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        if path is None:\n",
    "            self.model = AutoModel.from_pretrained(model_name).to(DEVICE)\n",
    "        else:\n",
    "            self.model = torch.load(path).to(DEVICE)\n",
    "        self.pooling = MeanPooling(starting_state)\n",
    "        self.model.eval()\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def forward(self, loader):\n",
    "        activations = []\n",
    "        for _, t in tqdm(enumerate(loader)):\n",
    "            tokenized = self.tokenizer(t, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "            tokenized_device = batch_to_device(tokenized, DEVICE)\n",
    "\n",
    "            out = self.model(**tokenized_device, output_hidden_states=True).hidden_states[self.pooling.starting_state]\n",
    "            out_mean = self.pooling(out, tokenized_device[\"attention_mask\"])\n",
    "            \n",
    "            activations.append(out_mean)\n",
    "            \n",
    "        return torch.cat(activations, dim=0).detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0128cda",
   "metadata": {},
   "source": [
    "# Calculating activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f059c57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_electra = [\n",
    "    \"model_2024_06_02_21_23_18_stsb\",\n",
    "    \"model_2024_06_02_21_53_27_stsb\",\n",
    "    \"model_2024_06_02_22_26_42_stsb\",\n",
    "    \"model_2024_06_02_23_03_24_stsb\",\n",
    "    \"model_2024_06_02_23_44_17_stsb\",\n",
    "    \"model_2024_06_03_00_29_11_stsb\",\n",
    "    \"model_2024_06_03_01_18_02_stsb\",\n",
    "    \"model_2024_06_03_02_10_39_stsb\",\n",
    "    \"model_2024_06_03_03_06_35_stsb\",\n",
    "    \"model_2024_06_03_04_06_06_stsb\",\n",
    "    \"model_2024_06_03_05_08_52_stsb\",\n",
    "    \"model_2024_06_03_06_14_58_stsb\",\n",
    "    \"model_2024_06_03_07_24_29_stsb\",\n",
    "]\n",
    "\n",
    "def get_activations(models, model_name, skip_path=False):\n",
    "    activations = []\n",
    "    for i, m in enumerate(models):\n",
    "        curr_model = GenericModel(\n",
    "            starting_state=i, \n",
    "            model_name=model_name,\n",
    "            path=None if skip_path else f\"../output/{model_name.replace('/','-')}/mean/{i}_to_{i+1}/{m}.pt\",\n",
    "        )\n",
    "        act_i = curr_model.forward(loader)\n",
    "        activations.append(act_i)\n",
    "    return np.array(activations)\n",
    "\n",
    "activations_electra = get_activations(models_electra, \"google/electra-base-discriminator\")\n",
    "activations_electra_pretrained = get_activations(models_electra, \"google/electra-base-discriminator\", True)\n",
    "    \n",
    "print(len(activations_electra), activations_electra[0].shape)\n",
    "print(len(activations_electra_pretrained), activations_electra_pretrained[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb783998",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_bert = [\n",
    "    \"model_2024_06_02_21_33_44_stsb\",\n",
    "    \"model_2024_06_02_22_05_08_stsb\",\n",
    "    \"model_2024_06_02_22_39_46_stsb\",\n",
    "    \"model_2024_06_02_23_18_33_stsb\",\n",
    "    \"model_2024_06_03_00_01_25_stsb\",\n",
    "    \"model_2024_06_03_00_48_11_stsb\",\n",
    "    \"model_2024_06_03_01_38_30_stsb\",\n",
    "    \"model_2024_06_03_02_32_23_stsb\",\n",
    "    \"model_2024_06_03_03_29_45_stsb\",\n",
    "    \"model_2024_06_03_04_30_33_stsb\",\n",
    "    \"model_2024_06_03_05_34_41_stsb\",\n",
    "    \"model_2024_06_03_06_42_05_stsb\",\n",
    "    \"model_2024_06_03_07_52_58_stsb\",\n",
    "]\n",
    "\n",
    "activations_bert = get_activations(models_bert, \"bert-base-cased\")\n",
    "activations_bert_pretrained = get_activations(models_bert, \"bert-base-cased\", True)\n",
    "    \n",
    "print(len(activations_bert), activations_bert[0].shape)\n",
    "print(len(activations_bert_pretrained), activations_bert_pretrained[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d602de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_electra_gen = [\n",
    "    \"model_2024_06_02_21_29_21_stsb\",\n",
    "    \"model_2024_06_02_22_00_12_stsb\",\n",
    "    \"model_2024_06_02_22_34_12_stsb\",\n",
    "    \"model_2024_06_02_23_11_44_stsb\",\n",
    "    \"model_2024_06_02_23_53_33_stsb\",\n",
    "    \"model_2024_06_03_00_39_17_stsb\",\n",
    "    \"model_2024_06_03_01_28_54_stsb\",\n",
    "    \"model_2024_06_03_02_22_15_stsb\",\n",
    "    \"model_2024_06_03_03_18_57_stsb\",\n",
    "    \"model_2024_06_03_04_19_12_stsb\",\n",
    "    \"model_2024_06_03_05_22_42_stsb\",\n",
    "    \"model_2024_06_03_06_29_33_stsb\",\n",
    "    \"model_2024_06_03_07_39_51_stsb\",\n",
    "]\n",
    "\n",
    "activations_electra_gen = get_activations(models_electra_gen, \"google/electra-base-generator\")\n",
    "activations_electra_gen_pretrained = get_activations(models_electra_gen, \"google/electra-base-generator\", True)\n",
    "    \n",
    "print(len(activations_electra_gen), activations_electra_gen[0].shape)\n",
    "print(len(activations_electra_gen_pretrained), activations_electra_gen_pretrained[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d25ba8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_deberta_disc = [\n",
    "    \"model_2024_06_02_21_12_47_stsb\",\n",
    "    \"model_2024_06_02_21_41_45_stsb\",\n",
    "    \"model_2024_06_02_22_14_00_stsb\",\n",
    "    \"model_2024_06_02_22_49_31_stsb\",\n",
    "    \"model_2024_06_02_23_29_12_stsb\",\n",
    "    \"model_2024_06_03_00_12_57_stsb\",\n",
    "    \"model_2024_06_03_01_00_37_stsb\",\n",
    "    \"model_2024_06_03_01_51_54_stsb\",\n",
    "    \"model_2024_06_03_02_46_43_stsb\",\n",
    "    \"model_2024_06_03_03_45_01_stsb\",\n",
    "    \"model_2024_06_03_04_46_41_stsb\",\n",
    "    \"model_2024_06_03_05_51_43_stsb\",\n",
    "    \"model_2024_06_03_07_00_04_stsb\",\n",
    "    ]\n",
    "\n",
    "activations_deberta_disc = get_activations(models_deberta_disc, \"microsoft/deberta-v3-base\")\n",
    "activations_deberta_disc_pretrained = get_activations(models_deberta_disc, \"microsoft/deberta-v3-base\", True)\n",
    "    \n",
    "print(len(activations_deberta_disc), activations_deberta_disc[0].shape)\n",
    "print(len(activations_deberta_disc_pretrained), activations_deberta_disc_pretrained[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e410d40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "activations_deberta_pretrained = get_activations(models_deberta_disc, \"microsoft/deberta-base\", True)\n",
    "    \n",
    "print(len(activations_deberta_pretrained), activations_deberta_pretrained[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fcf7314",
   "metadata": {},
   "source": [
    "# ELECTRA discriminator VS BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39cd1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_cka_on_diag(model_acts1, model_acts2, rbf_val=None):\n",
    "    cka_values = []\n",
    "    for i in tqdm(range(len(model_acts1))):\n",
    "        m1 = model_acts1[i]\n",
    "        m2 = model_acts2[i]\n",
    "        if rbf_val is not None:\n",
    "            cka_values.append(cka(gram_rbf(m1, rbf_val), gram_rbf(m2, rbf_val)))\n",
    "        else:\n",
    "            cka_values.append(cka(gram_linear(m1), gram_linear(m2)))\n",
    "\n",
    "    return cka_values\n",
    "\n",
    "def plot_diag(diags, labels, model_name1, model_name2, xlabel=\"Layer index\", ylabel=\"CKA value\", skip_y=False):\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
    "    ax.set_facecolor(\"#F9F9F9\")\n",
    "    ax.grid(color='black', linestyle=(0, (1, 5)), linewidth=0.5)\n",
    "    for d, l in zip(diags, labels):\n",
    "        ax.plot(d, \"-o\", label=l)\n",
    "    plt.legend(fontsize=14)\n",
    "    #ax.set_title(f\"{model_name1} and {model_name2}\")\n",
    "    ax.set_ylim([-0.05, 1.05])\n",
    "    ax.set_xlabel(xlabel, fontsize=14)\n",
    "    if not skip_y:\n",
    "        ax.set_ylabel(ylabel, fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b4e925",
   "metadata": {},
   "outputs": [],
   "source": [
    "diag = do_cka_on_diag(activations_electra, activations_bert)\n",
    "diag_pretrained = do_cka_on_diag(activations_electra_pretrained, activations_bert_pretrained)\n",
    "\n",
    "plot_diag(\n",
    "    diags=[diag_pretrained, diag],\n",
    "    labels=[\n",
    "        \"Pre-trained\",\n",
    "        \"Fine-tuned on STSB\"\n",
    "    ],\n",
    "    model_name1=\"BERT$_{base}$ cased\",\n",
    "    model_name2=\"ELECTRA$_{base}$ discriminator\",\n",
    "    skip_y=True,\n",
    ")\n",
    "plt.savefig(\"./plots/electra_disc_bert.pdf\", format=\"pdf\", bbox_inches=\"tight\", dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d003991",
   "metadata": {},
   "source": [
    "# ELECTRA generator VS BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7993a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "diag = do_cka_on_diag(activations_electra_gen, activations_bert)\n",
    "diag_pretrained = do_cka_on_diag(activations_electra_gen_pretrained, activations_bert_pretrained)\n",
    "\n",
    "plot_diag(\n",
    "    diags=[diag_pretrained, diag],\n",
    "    labels=[\n",
    "        \"Pre-trained\",\n",
    "        \"Fine-tuned on STSB\"\n",
    "    ],\n",
    "    model_name1=\"BERT$_{base}$ cased\",\n",
    "    model_name2=\"ELECTRA$_{base}$ generator\",\n",
    ")\n",
    "plt.savefig(\"./plots/electra_gen_bert.pdf\", format=\"pdf\", bbox_inches=\"tight\", dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b8ad20",
   "metadata": {},
   "source": [
    "# DeBERTa discriminator VS BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbbe571",
   "metadata": {},
   "outputs": [],
   "source": [
    "diag = do_cka_on_diag(activations_deberta_disc, activations_bert)\n",
    "diag_pretrained = do_cka_on_diag(activations_deberta_disc_pretrained, activations_bert_pretrained)\n",
    "\n",
    "plot_diag(\n",
    "    diags=[diag_pretrained, diag],\n",
    "    labels=[\n",
    "        \"Pre-trained\",\n",
    "        \"Fine-tuned on STSB\"\n",
    "    ],\n",
    "    model_name1=\"BERT$_{base}$ cased\",\n",
    "    model_name2=\"DeBERTa-V3$_{base}$ discriminator\",\n",
    "    skip_y=True,\n",
    ")\n",
    "plt.savefig(\"./plots/deberta_disc_bert.pdf\", format=\"pdf\", bbox_inches=\"tight\", dpi=300)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
