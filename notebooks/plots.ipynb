{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f9c7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf4d4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_default = [\n",
    "    (\"bert-base-cased\", [\"mean\"], 13, \n",
    "         [\n",
    "            (\"test_results_stsb\", \"BERT$_{base}$ cased\"),\n",
    "         ]\n",
    "    ),\n",
    "    (\"google-electra-base-discriminator\", [\"mean\"], 13, \n",
    "         [\n",
    "            (\"test_results_stsb\", \"ELECTRA$_{base}$ discriminator\"),\n",
    "         ]\n",
    "    ),\n",
    "    (\"google-electra-base-generator\", [\"mean\"], 13, \n",
    "         [\n",
    "            (\"test_results_stsb\", \"ELECTRA$_{base}$ generator\"),\n",
    "         ]\n",
    "    ),\n",
    "    (\"microsoft-deberta-v3-base\", [\"mean\"], 13, \n",
    "         [\n",
    "            (\"test_results_stsb\", \"DeBERTA-V3$_{base}$ discriminator\"),\n",
    "         ]\n",
    "    )\n",
    "]\n",
    "\n",
    "models_wordsim = [\n",
    "    (\"bert-base-cased\", [\"mean\"], 13, \n",
    "         [\n",
    "            (\"test_resultsword_similarity_stsb\", \"BERT$_{base}$ cased\"),\n",
    "         ]\n",
    "    ),\n",
    "    (\"google-electra-base-discriminator\", [\"mean\"], 13, \n",
    "         [\n",
    "            (\"test_resultsword_similarity_stsb\", \"ELECTRA$_{base}$ discriminator\"),\n",
    "         ]\n",
    "    ),\n",
    "    (\"google-electra-base-generator\", [\"mean\"], 13, \n",
    "         [\n",
    "            (\"test_resultsword_similarity_stsb\", \"ELECTRA$_{base}$ generator\"),\n",
    "         ]\n",
    "    ),\n",
    "    (\"microsoft-deberta-v3-base\", [\"mean\"], 13, \n",
    "         [\n",
    "            (\"test_resultsword_similarity_stsb\", \"DeBERTA-V3$_{base}$ discriminator\"),\n",
    "         ]\n",
    "    )\n",
    "]\n",
    "\n",
    "\n",
    "models_mlm = [\n",
    "    (\"bert-base-cased\", [\"mean\"], 13, \n",
    "         [\n",
    "            (\"test_results_bert-base-cased_model_epoch_9_mlm_stsb\", \"BERT$_{base}$ cased\"),\n",
    "         ]\n",
    "    ),\n",
    "    (\"google-electra-base-discriminator\", [\"mean\"], 13, \n",
    "         [\n",
    "            (\"test_results_google-electra-base-discriminator_model_epoch_9_mlm_stsb\", \"ELECTRA$_{base}$ discriminator\"),\n",
    "         ]\n",
    "    ),\n",
    "    (\"google-electra-base-generator\", [\"mean\"], 13, \n",
    "         [\n",
    "            (\"test_results_google-electra-base-generator_model_epoch_9_mlm_stsb\", \"ELECTRA$_{base}$ generator\"),\n",
    "         ]\n",
    "    ),\n",
    "    (\"microsoft-deberta-v3-base\", [\"mean\"], 13, \n",
    "         [\n",
    "            (\"test_results_microsoft-deberta-v3-base_model_epoch_9_mlm_stsb\", \"DeBERTA-V3$_{base}$ discriminator\"),\n",
    "         ]\n",
    "    )\n",
    "]\n",
    "\n",
    "models = [\n",
    "    models_default, models_wordsim, models_mlm\n",
    "]\n",
    "colors=[\n",
    "    [None, None, None, None]  for i in range(3)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f3da22",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def add_to_plot(models, colors, ax, plot_idx, ylim_spearman=[53, 87]):\n",
    "    ax.set_facecolor(\"#F9F9F9\")\n",
    "    ax.grid(color='black', linestyle=(0, (1, 5)), linewidth=0.5)\n",
    "    for x, color in zip(models, colors):\n",
    "        m = x[0]\n",
    "        pool = x[1]\n",
    "        c = x[2]\n",
    "        json_names_labels = x[3]\n",
    "        for p in pool:\n",
    "            for (name, label) in json_names_labels:\n",
    "                spearman, pearson = [], []\n",
    "                std_spearman, std_pearson = [], []\n",
    "                for i in range(c):\n",
    "                    res = json.load(open(f\"../output/{m}/{p}/{i}_to_{i+1}/{name}.json\"))\n",
    "                    \n",
    "                    std_1 = np.array(res[\"stdev_cosine_spearman_test\"]) * 100\n",
    "                    mean_1 = res[\"mean_cosine_spearman_test\"] * 100\n",
    "                    std_spearman.append((mean_1-std_1, mean_1+std_1))\n",
    "                    spearman.append(mean_1)\n",
    "\n",
    "                    std_2 = np.array(res[\"stdev_cosine_pearson_test\"]) * 100\n",
    "                    mean_2 = res[\"mean_cosine_pearson_test\"] * 100\n",
    "                    std_pearson.append((mean_2-std_2, mean_2+std_2))\n",
    "                    pearson.append(mean_2)\n",
    "\n",
    "                ax.plot(spearman, \"-o\", label=label)\n",
    "                ax.fill_between(\n",
    "                    np.array(list(range(c))), \n",
    "                    np.array(std_spearman)[:,0], \n",
    "                    np.array(std_spearman)[:,1],\n",
    "                    alpha=0.2\n",
    "                )\n",
    "                ax.set_xlabel(\"Layer index\", fontsize=14)\n",
    "                if plot_idx == 0:\n",
    "                    ax.set_ylabel(\"Test set Spearman correlation coefficient ($\\\\rho$ $\\\\times$ 100)\", fontsize=14)\n",
    "                else:\n",
    "                    ax.set_ylabel(\"\", fontsize=14)\n",
    "                ax.legend(loc=\"lower right\", fontsize=14)\n",
    "                if ylim_spearman is not None:\n",
    "                    ax.set_ylim(ylim_spearman)\n",
    "\n",
    "                    \n",
    "names = [\n",
    "    \"./plots/comparison_plot.pdf\",\n",
    "    \"./plots/comparison_plot_wordsim.pdf\",\n",
    "    \"./plots/comparison_plot_mlm.pdf\",\n",
    "]\n",
    "\n",
    "for i in range(len(names)):\n",
    "    fig, axs = plt.subplots(1, 1, figsize=(8, 8))\n",
    "    for m, c, ax in zip([models[i]], [colors[i]], [axs]):\n",
    "        add_to_plot(m, c, ax, i)\n",
    "    plt.savefig(names[i], format=\"pdf\", bbox_inches=\"tight\", dpi=300)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659993cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_default = [\n",
    "    (\"bert-base-cased\", [\"mean\"], 13, \n",
    "         [\n",
    "            (\"test_results_random_stsb\", \"BERT$_{base}$ cased\"),\n",
    "         ]\n",
    "    ),\n",
    "    (\"google-electra-base-discriminator\", [\"mean\"], 13, \n",
    "         [\n",
    "            (\"test_results_random_stsb\", \"ELECTRA$_{base}$ discriminator\"),\n",
    "         ]\n",
    "    ),\n",
    "    (\"google-electra-base-generator\", [\"mean\"], 13, \n",
    "         [\n",
    "            (\"test_results_random_stsb\", \"ELECTRA$_{base}$ generator\"),\n",
    "         ]\n",
    "    ),\n",
    "    (\"microsoft-deberta-v3-base\", [\"mean\"], 13, \n",
    "         [\n",
    "            (\"test_results_random_stsb\", \"DeBERTA-V3$_{base}$ discriminator\"),\n",
    "         ]\n",
    "    )\n",
    "]\n",
    "\n",
    "models = [\n",
    "    models_default\n",
    "]\n",
    "colors=[\n",
    "    [None, None, None, None]  for i in range(3)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288390de",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [\n",
    "    \"./plots/comparison_plot_random.pdf\",\n",
    "]\n",
    "\n",
    "for i in range(len(names)):\n",
    "    fig, axs = plt.subplots(1, 1, figsize=(8, 8))\n",
    "    for m, c, ax in zip([models[i]], [colors[i]], [axs]):\n",
    "        add_to_plot(m, c, ax, i, ylim_spearman=[40, 75])\n",
    "    plt.savefig(names[i], format=\"pdf\", bbox_inches=\"tight\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a2a900",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_default = [\n",
    "    (\"bert-base-cased\", [\"mean\"], 13, \n",
    "         [\n",
    "            (\"test_results_mrpc_mrpc\", \"BERT$_{base}$ cased\"),\n",
    "         ]\n",
    "    ),\n",
    "    (\"google-electra-base-discriminator\", [\"mean\"], 13, \n",
    "         [\n",
    "            (\"test_results_mrpc_mrpc\", \"ELECTRA$_{base}$ discriminator\"),\n",
    "         ]\n",
    "    ),\n",
    "    (\"google-electra-base-generator\", [\"mean\"], 13, \n",
    "         [\n",
    "            (\"test_results_mrpc_mrpc\", \"ELECTRA$_{base}$ generator\"),\n",
    "         ]\n",
    "    ),\n",
    "    (\"microsoft-deberta-v3-base\", [\"mean\"], 13, \n",
    "         [\n",
    "            (\"test_results_mrpc_mrpc\", \"DeBERTA-V3$_{base}$ discriminator\"),\n",
    "         ]\n",
    "    )\n",
    "]\n",
    "\n",
    "models = [\n",
    "    models_default\n",
    "]\n",
    "colors=[\n",
    "    [None, None, None, None]  for i in range(3)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd6c20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_plot_mrpc(models, colors, ax, plot_idx, ylim_f1=[53, 95], add_std=True):\n",
    "    ax.set_facecolor(\"#F9F9F9\")\n",
    "    ax.grid(color='black', linestyle=(0, (1, 5)), linewidth=0.5)\n",
    "    for x, color in zip(models, colors):\n",
    "        m = x[0]\n",
    "        pool = x[1]\n",
    "        c = x[2]\n",
    "        json_names_labels = x[3]\n",
    "        for p in pool:\n",
    "            for (name, label) in json_names_labels:\n",
    "                f1 = []\n",
    "                std_f1 = []\n",
    "                for i in range(c):\n",
    "                    res = json.load(open(f\"../output/{m}/{p}/{i}_to_{i+1}/{name}.json\"))\n",
    "                    \n",
    "                    std_1 = np.array(res[\"stdev_cosine_f1_test\"]) * 100\n",
    "                    mean_1 = res[\"mean_cosine_f1_test\"] * 100\n",
    "                    std_f1.append((mean_1-std_1, mean_1+std_1))\n",
    "                    f1.append(mean_1)\n",
    "\n",
    "                ax.plot(f1, \"-o\", label=label)\n",
    "                if add_std:\n",
    "                    ax.fill_between(\n",
    "                        np.array(list(range(c))), \n",
    "                        np.array(std_f1)[:,0], \n",
    "                        np.array(std_f1)[:,1],\n",
    "                        alpha=0.2\n",
    "                    )\n",
    "                ax.set_xlabel(\"Layer index\", fontsize=14)\n",
    "                if plot_idx == 0:\n",
    "                    ax.set_ylabel(\"Test set F1 score\", fontsize=14)\n",
    "                else:\n",
    "                    ax.set_ylabel(\"\", fontsize=14)\n",
    "                ax.legend(loc=\"lower right\", fontsize=14)\n",
    "                if ylim_f1 is not None:\n",
    "                    ax.set_ylim(ylim_f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dbb8574",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [\n",
    "    \"./plots/comparison_plot_mrpc.pdf\",\n",
    "]\n",
    "\n",
    "for i in range(len(names)):\n",
    "    fig, axs = plt.subplots(1, 1, figsize=(8, 8))\n",
    "    for m, c, ax in zip([models[i]], [colors[i]], [axs]):\n",
    "        add_to_plot_mrpc(m, c, ax, i)\n",
    "    plt.savefig(names[i], format=\"pdf\", bbox_inches=\"tight\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ff3de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_default = [\n",
    "    (\"bert-base-cased\", [\"mean\"], 13, \n",
    "         [\n",
    "            (\"test_results_sick_diff_concatenation_sick\", \"BERT$_{base}$ cased\"),\n",
    "         ]\n",
    "    ),\n",
    "    (\"google-electra-base-discriminator\", [\"mean\"], 13, \n",
    "         [\n",
    "            (\"test_results_sick_diff_concatenation_sick\", \"ELECTRA$_{base}$ discriminator\"),\n",
    "         ]\n",
    "    ),\n",
    "    (\"google-electra-base-generator\", [\"mean\"], 13, \n",
    "         [\n",
    "            (\"test_results_sick_diff_concatenation_sick\", \"ELECTRA$_{base}$ generator\"),\n",
    "         ]\n",
    "    ),\n",
    "    (\"microsoft-deberta-v3-base\", [\"mean\"], 13, \n",
    "         [\n",
    "            (\"test_results_sick_diff_concatenation_sick\", \"DeBERTA-V3$_{base}$ discriminator\"),\n",
    "         ]\n",
    "    )\n",
    "]\n",
    "\n",
    "models = [\n",
    "    models_default\n",
    "]\n",
    "colors=[\n",
    "    [None, None, None, None]  for i in range(3)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ade3e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_plot_sick_multi(models, colors, ax, plot_idx, ylim_f1=[45, 75], add_std=True):\n",
    "    ax.set_facecolor(\"#F9F9F9\")\n",
    "    ax.grid(color='black', linestyle=(0, (1, 5)), linewidth=0.5)\n",
    "    for x, color in zip(models, colors):\n",
    "        m = x[0]\n",
    "        pool = x[1]\n",
    "        c = x[2]\n",
    "        json_names_labels = x[3]\n",
    "        for p in pool:\n",
    "            for (name, label) in json_names_labels:\n",
    "                f1 = []\n",
    "                std_f1 = []\n",
    "                for i in range(c):\n",
    "                    res = json.load(open(f\"../output/{m}/{p}/{i}_to_{i+1}/{name}.json\"))\n",
    "                    \n",
    "                    std_1 = np.array(res[\"stdev_diff_concatenation_f1_test\"]) * 100\n",
    "                    mean_1 = res[\"mean_diff_concatenation_f1_test\"] * 100\n",
    "                    std_f1.append((mean_1-std_1, mean_1+std_1))\n",
    "                    f1.append(mean_1)\n",
    "\n",
    "                ax.plot(f1, \"-o\", label=label)\n",
    "                if add_std:\n",
    "                    ax.fill_between(\n",
    "                        np.array(list(range(c))), \n",
    "                        np.array(std_f1)[:,0], \n",
    "                        np.array(std_f1)[:,1],\n",
    "                        alpha=0.2\n",
    "                    )\n",
    "                ax.set_xlabel(\"Layer index\", fontsize=14)\n",
    "                if plot_idx == 0:\n",
    "                    ax.set_ylabel(\"Test set F1 score\", fontsize=14)\n",
    "                else:\n",
    "                    ax.set_ylabel(\"\", fontsize=14)\n",
    "                ax.legend(loc=\"lower right\", fontsize=14)\n",
    "                if ylim_f1 is not None:\n",
    "                    ax.set_ylim(ylim_f1)\n",
    "\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89312fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "names = [\n",
    "    \"./plots/comparison_plot_sick_multi.pdf\",\n",
    "]\n",
    "\n",
    "for i in range(len(names)):\n",
    "    fig, axs = plt.subplots(1, 1, figsize=(8, 8))\n",
    "    for m, c, ax in zip([models[i]], [colors[i]], [axs]):\n",
    "        add_to_plot_sick_multi(m, c, ax, i, [20, 80], add_std=True)\n",
    "    plt.savefig(names[i], format=\"pdf\", bbox_inches=\"tight\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8cc716",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "models = [\n",
    "    [\n",
    "        (\"bert-base-cased\", 13, \"BERT$_{base}$\"),\n",
    "        (\"bert-large-cased\", 25, \"BERT$_{large}$\"),\n",
    "        (\"google-bert_uncased_L-8_H-512_A-8\", 9, \"BERT$_{medium}$\", (0, -0.3)),\n",
    "        (\"google-bert_uncased_L-4_H-512_A-8\", 5,\"BERT$_{small}$\"),\n",
    "        (\"google-bert_uncased_L-2_H-128_A-2\", 3,\"BERT$_{tiny}$\"),\n",
    "        (\"google-bert_uncased_L-4_H-256_A-4\", 5,\"BERT$_{mini}$\"),\n",
    "    ],\n",
    "    [\n",
    "        (\"google-electra-small-discriminator\", 13, \"ELECTRA$_{D\\:small}$\"),\n",
    "        (\"google-electra-base-discriminator\", 13, \"ELECTRA$_{D\\:base}$\", (0, -0.5)),\n",
    "        (\"google-electra-large-discriminator\", 25, \"ELECTRA$_{D\\:large}$\"),\n",
    "    ],\n",
    "    [\n",
    "        (\"google-electra-small-generator\", 13, \"ELECTRA$_{G\\:small}$\", (0, -0.3)),\n",
    "        (\"google-electra-base-generator\", 13, \"ELECTRA$_{G\\:base}$\", (0, -0.2)),\n",
    "        (\"google-electra-large-generator\", 25, \"ELECTRA$_{G\\:large}$\"),\n",
    "\n",
    "    ]\n",
    "]\n",
    "\n",
    "\n",
    "def count_params(model, i):\n",
    "    x = torch.tensor(0.)\n",
    "    for p in model.embeddings.parameters():\n",
    "        x += torch.prod(torch.tensor(p.shape))\n",
    "    if i > 0:\n",
    "        for p in model.encoder.layer[:i].parameters():\n",
    "            x += torch.prod(torch.tensor(p.shape))\n",
    "    return x/10**6\n",
    "\n",
    "all_scores_params = []\n",
    "plt.figure(figsize=(8, 8))\n",
    "ax = plt.gca()\n",
    "for f in models:\n",
    "    val_spearman, spearman, parameters = [], [], []\n",
    "     \n",
    "    if \"discriminator\" in f[0][0]:\n",
    "        disc_spearman, disc_params = [], []\n",
    "   \n",
    "    for m in tqdm(f):\n",
    "        val_means, means = [], []\n",
    "        name = m[0] if \"google\" not in m[0] else m[0].replace(\"google-\", \"google/\")\n",
    "        \n",
    "        try:\n",
    "            model = AutoModel.from_pretrained(name, add_pooling_layer=False)\n",
    "        except TypeError:\n",
    "            model = AutoModel.from_pretrained(name)\n",
    "            \n",
    "        for i in range(m[1]):\n",
    "            params = count_params(model, i)\n",
    "            res = json.load(open(f\"../output/{m[0]}/mean/{i}_to_{i+1}/test_results.json\"))\n",
    "            means.append([res[\"mean_cosine_spearman_test\"]*100, params])\n",
    "            res = json.load(open(f\"../output/{m[0]}/mean/{i}_to_{i+1}/val_results.json\"))\n",
    "            val_means.append(res[\"mean_cosine_spearman_val\"])\n",
    "\n",
    "        if \"discriminator\" in m[0] and \"large\" not in m[0]:\n",
    "            x, y = means[-1]\n",
    "            disc_spearman.append(x)\n",
    "            disc_params.append(y)\n",
    "            split_idx = m[2].rfind('}')\n",
    "            plt.annotate(m[2][:split_idx] + \"\\: last\" + m[2][split_idx:], (y+5, x-0.5), fontsize=12)\n",
    "            \n",
    "        \n",
    "        indices = np.argsort(val_means)\n",
    "        means = np.array(means)[indices]\n",
    "        x, y = means[::-1, :][0]\n",
    "        all_scores_params.append([x, y])\n",
    "        spearman.append(x)\n",
    "        parameters.append(y)\n",
    "        x_delta, y_delta = m[3] if len(m) == 4 else (0, 0)\n",
    "        plt.annotate(m[2], (y+5+x_delta, x-0.002+y_delta), fontsize=12)\n",
    "\n",
    "    means_ = np.array([spearman, parameters]).T\n",
    "    means_.sort(axis=0)\n",
    "    \n",
    "    if \"discriminator\" in m[0]:\n",
    "        plt.plot(disc_params,disc_spearman, \"-o\", c=\"r\")\n",
    "\n",
    "    plt.plot(means_[:,1], means_[:,0], \"-o\")\n",
    "    plt.xlim([plt.xlim()[0], plt.xlim()[1]+15])\n",
    "    plt.xlabel(\"Number of parameters ($10^6$)\", fontsize=14)\n",
    "    plt.ylabel(\"Test set Spearman correlation coefficient ($\\\\rho$ $\\\\times$ 100)\", fontsize=14)\n",
    "ax.set_facecolor(\"#F9F9F9\")\n",
    "ax.grid(color='black', linestyle=(0, (1, 5)), linewidth=0.5)\n",
    "\n",
    "\n",
    "def pareto_front(points):\n",
    "    mask = np.ones(len(points), dtype=bool)\n",
    "    for i, (x, y) in enumerate(points):\n",
    "        for j, (x_other, y_other) in enumerate(points):\n",
    "            if i != j:\n",
    "                if x_other <= x and y_other >= y:\n",
    "                    mask[i] = False\n",
    "                    break \n",
    "    return mask\n",
    "\n",
    "all_scores_params = np.array(all_scores_params)\n",
    "all_scores_params[:,[0,1]] = all_scores_params[:,[1,0]]\n",
    "pareto = all_scores_params[pareto_front(all_scores_params)].tolist()\n",
    "pareto = np.array(sorted(pareto, key=lambda x: (x[0], x[1])))\n",
    "plt.plot(pareto[:,0], pareto[:, 1], \"-o\", color=\"gray\", alpha=0.25, linewidth=10, zorder=0)\n",
    "\n",
    "\n",
    "plt.savefig(\"./plots/pareto.pdf\", format=\"pdf\", bbox_inches=\"tight\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7296fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    (\"bert-base-cased\", 13),\n",
    "    (\"bert-large-cased\", 25),\n",
    "    (\"google/bert_uncased_L-8_H-512_A-8\", 9),\n",
    "    (\"google/bert_uncased_L-4_H-512_A-8\", 5),\n",
    "    (\"google/bert_uncased_L-2_H-128_A-2\", 3),\n",
    "    (\"google/bert_uncased_L-4_H-256_A-4\", 5),\n",
    "    (\"google/electra-small-discriminator\", 13),\n",
    "    (\"google/electra-base-discriminator\", 13),\n",
    "    (\"google/electra-base-discriminator\", 12),\n",
    "    (\"google/electra-base-discriminator\", 10),\n",
    "    (\"google/electra-base-discriminator\", 4),\n",
    "    (\"google/electra-large-discriminator\", 25),\n",
    "    (\"google/electra-small-generator\", 13),\n",
    "    (\"google/electra-base-generator\", 13),\n",
    "    (\"google/electra-large-generator\", 25),\n",
    "    (\"microsoft/deberta-v3-base\", 9),\n",
    "    (\"microsoft/deberta-v3-base\", 13),\n",
    "]\n",
    "\n",
    "\n",
    "for x in models:\n",
    "    m, states = x\n",
    "    try:\n",
    "        model = AutoModel.from_pretrained(m, add_pooling_layer=False)\n",
    "    except TypeError:\n",
    "        model = AutoModel.from_pretrained(m)\n",
    "    # states - 1 since in upper loop the final value isnt used\n",
    "    print(m, count_params(model, states-1).item(), states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992cde25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_func(models, ylim_spearman=[0.53, 0.87], legend_loc=\"best\"):\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
    "\n",
    "    for x in models:\n",
    "        m = x[0]\n",
    "        pool = x[1]\n",
    "        c = x[2]\n",
    "        json_names_labels = x[3]\n",
    "        for p in pool:\n",
    "            for (name, label) in json_names_labels:\n",
    "                spearman = []\n",
    "                std_spearman = []\n",
    "                for i in range(c):\n",
    "                    res = json.load(open(f\"../output/{m}/{p}/{i}_to_{i+1}/{name}.json\"))\n",
    "\n",
    "                    std_1 = res[\"stdev_cosine_spearman_test\"] * 100\n",
    "                    mean_1 = res[\"mean_cosine_spearman_test\"] * 100\n",
    "                    std_spearman.append((mean_1-std_1, mean_1+std_1))\n",
    "                    spearman.append(mean_1)\n",
    "\n",
    "                ax.plot(spearman, \"-o\", label=label)\n",
    "                if \"unsupervised\" not in name:\n",
    "                    ax.fill_between(\n",
    "                        np.array(list(range(c))), \n",
    "                        np.array(std_spearman)[:,0], \n",
    "                        np.array(std_spearman)[:,1], \n",
    "                        alpha=0.15\n",
    "                    )\n",
    "                ax.set_xlabel(\"Layer index\", fontsize=14)\n",
    "                ax.set_ylabel(\"Test set Spearman correlation coefficient ($\\\\rho$ $\\\\times$ 100)\", fontsize=14)\n",
    "                ax.legend(loc=legend_loc, fontsize=14)\n",
    "                if ylim_spearman is not None:\n",
    "                    ax.set_ylim(ylim_spearman)\n",
    "                ax.set_facecolor(\"#F9F9F9\")\n",
    "                ax.grid(color='black', linestyle=(0, (1, 5)), linewidth=0.5)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6602370",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    (\"google-electra-small-discriminator\", [\"mean\"], 13,\n",
    "         [\n",
    "            (\"test_results_stsb\", \"ELECTRA$_{small}$ discriminator\"),\n",
    "         ]\n",
    "    ),\n",
    "    (\"google-electra-base-discriminator\", [\"mean\"], 13,\n",
    "         [\n",
    "            (\"test_results_stsb\", \"ELECTRA$_{base}$ discriminator\"),\n",
    "         ]\n",
    "    ),\n",
    "    (\"google-electra-large-discriminator\", [\"mean\"], 25,\n",
    "        [\n",
    "            (\"test_results_stsb\", \"ELECTRA$_{large}$ discriminator\"),\n",
    "         ]\n",
    "    )\n",
    "]\n",
    "\n",
    "plot_func(models, ylim_spearman=None, legend_loc=\"lower left\")\n",
    "plt.savefig(\"./plots/discriminator.pdf\", format=\"pdf\", bbox_inches=\"tight\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9e10c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    (\"google-electra-small-generator\", [\"mean\"], 13,\n",
    "         [\n",
    "            (\"test_results_stsb\", \"ELECTRA$_{small}$ generator\"),\n",
    "         ]\n",
    "    ),\n",
    "    (\"google-electra-base-generator\", [\"mean\"], 13,\n",
    "         [\n",
    "            (\"test_results_stsb\", \"ELECTRA$_{base}$ generator\"),\n",
    "         ]\n",
    "    ),\n",
    "    (\"google-electra-large-generator\", [\"mean\"], 25,\n",
    "        [\n",
    "            (\"test_results_stsb\", \"ELECTRA$_{large}$ generator\"),\n",
    "         ]\n",
    "    )\n",
    "]\n",
    "\n",
    "plot_func(models, ylim_spearman=None)\n",
    "plt.savefig(\"./plots/generator.pdf\", format=\"pdf\", bbox_inches=\"tight\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce77b041",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    \n",
    "    (\"bert-base-cased\", [\"mean\"], 13,\n",
    "         [\n",
    "            (\"test_results_stsb\", \"BERT$_{base}$ cased\"),\n",
    "         ]\n",
    "    ),\n",
    "    (\"bert-large-cased\", [\"mean\"], 25,\n",
    "         [\n",
    "            (\"test_results_stsb\", \"BERT$_{large}$ cased\"),\n",
    "         ]\n",
    "    ),\n",
    "    (\"google-bert_uncased_L-2_H-128_A-2\", [\"mean\"], 3,\n",
    "        [\n",
    "            (\"test_results_stsb\", \"BERT$_{tiny}$\"),\n",
    "         ]\n",
    "    ),\n",
    "    (\"google-bert_uncased_L-4_H-256_A-4\", [\"mean\"], 5,\n",
    "        [\n",
    "            (\"test_results_stsb\", \"BERT$_{mini}$\"),\n",
    "         ]\n",
    "    ),\n",
    "    (\"google-bert_uncased_L-4_H-512_A-8\", [\"mean\"], 5,\n",
    "        [\n",
    "            (\"test_results_stsb\", \"BERT$_{small}$\"),\n",
    "         ]\n",
    "    ),\n",
    "    (\"google-bert_uncased_L-8_H-512_A-8\", [\"mean\"], 9,\n",
    "        [\n",
    "            (\"test_results_stsb\", \"BERT$_{medium}$\"),\n",
    "         ]\n",
    "    ),\n",
    "]\n",
    "\n",
    "plot_func(models, ylim_spearman=None)\n",
    "plt.savefig(\"./plots/bert.pdf\", format=\"pdf\", bbox_inches=\"tight\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f475f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    (\"klue-bert-base\", [\"mean\"], 13, \n",
    "        [\n",
    "            (\"test_results_kor_sts_kor_sts\", \"KLUE BERT$_{base}$\"),\n",
    "         ]\n",
    "    ),\n",
    "    (\"monologg-koelectra-base-v3-discriminator\", [\"mean\"], 13,\n",
    "        [\n",
    "            (\"test_results_kor_sts_kor_sts\", \"KoELECTRA$_{base}$ discriminator\"),\n",
    "         ]\n",
    "    ),\n",
    "    (\"monologg-koelectra-base-v3-generator\", [\"mean\"], 13, \n",
    "        [\n",
    "            (\"test_results_kor_sts_kor_sts\", \"KoELECTRA$_{base}$ generator\")\n",
    "         ]\n",
    "    ),\n",
    "]\n",
    "\n",
    "plot_func(models, ylim_spearman=None)\n",
    "plt.savefig(\"./plots/korean.pdf\", format=\"pdf\", bbox_inches=\"tight\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a6c4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    (\"google-bert-bert-base-german-cased\", [\"mean\"], 13,\n",
    "        [\n",
    "            (\"test_results_german_sts_german_sts\", \"BERT$_{base}$ german cased\"),\n",
    "         ]\n",
    "    ),\n",
    "    (\"deepset-gelectra-base\", [\"mean\"], 13, \n",
    "        [\n",
    "            (\"test_results_german_sts_german_sts\", \"GELECTRA$_{base}$ discriminator\"),\n",
    "         ]\n",
    "    ),\n",
    "    (\"deepset-gelectra-base-generator\", [\"mean\"], 13, \n",
    "        [\n",
    "            (\"test_results_german_sts_german_sts\", \"GELECTRA$_{base}$ generator\"),\n",
    "         ]\n",
    "    ),\n",
    "]\n",
    "\n",
    "plot_func(models, ylim_spearman=None)\n",
    "plt.savefig(\"./plots/german.pdf\", format=\"pdf\", bbox_inches=\"tight\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3829f0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    (\"dccuchile-bert-base-spanish-wwm-cased\", [\"mean\"], 13, \n",
    "        [\n",
    "            (\"test_results_spanish_sts_spanish_sts\", \"BETO$_{base}$\"),\n",
    "         ]\n",
    "    ),\n",
    "    (\"mrm8488-electricidad-base-discriminator\", [\"mean\"], 13, \n",
    "        [\n",
    "            (\"test_results_spanish_sts_spanish_sts\", \"Electricidad$_{base}$ discriminator\"),\n",
    "         ]\n",
    "    ),\n",
    "    (\"mrm8488-electricidad-base-generator\", [\"mean\"], 13, \n",
    "        [\n",
    "            (\"test_results_spanish_sts_spanish_sts\", \"Electricidad$_{base}$ generator\"),\n",
    "         ]\n",
    "    ),\n",
    "]\n",
    "\n",
    "plot_func(models, ylim_spearman=None)\n",
    "plt.savefig(\"./plots/spanish.pdf\", format=\"pdf\", bbox_inches=\"tight\", dpi=300)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
