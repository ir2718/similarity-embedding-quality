{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06e2ec23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import senteval\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import json\n",
    "import os\n",
    "\n",
    "fs = os.listdir(\"..\")\n",
    "if not \"SentEval\" in fs:\n",
    "    %cd ./..\n",
    "    !git clone git@github.com:facebookresearch/SentEval.git\n",
    "    %cd notebooks\n",
    "    %mkdir data\n",
    "\n",
    "PATH_TO_DATA = \"../SentEval/data/probing/\"\n",
    "DEVICE = \"cuda:1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30bd8d42",
   "metadata": {},
   "source": [
    "# BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea24af42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-09 00:18:30.347945: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-09 00:18:30.347977: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-09 00:18:30.348005: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-09 00:18:30.353796: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Length': {'devacc': 69.92, 'acc': 70.21, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 57.66, 'acc': 57.61, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 31.21, 'acc': 31.33, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 63.47, 'acc': 63.54, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 83.46, 'acc': 82.65, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 88.26, 'acc': 86.16, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 82.22, 'acc': 80.76, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 77.1, 'acc': 78.45, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 64.27, 'acc': 64.33, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 66.7, 'acc': 67.01, 'ndev': 10002, 'ntest': 10002}}\n"
     ]
    }
   ],
   "source": [
    "def batch_to_device(d, device):\n",
    "    return {k: v.to(device) for k, v in d.items()}\n",
    "    \n",
    "class MeanPooling(nn.Module):\n",
    "    def __init__(self, starting_state):\n",
    "        super().__init__()\n",
    "        self.starting_state = starting_state\n",
    "\n",
    "    def forward(self, x, attention_mask):\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(x.size()).float()\n",
    "        emb_sum = torch.sum(x * input_mask_expanded, dim=1)\n",
    "        sum_mask = torch.clamp(input_mask_expanded.sum(dim=1), min=1e-9) # denominator\n",
    "        emb_mean = emb_sum / sum_mask\n",
    "        return emb_mean\n",
    "\n",
    "    \n",
    "class Bert:\n",
    "\n",
    "    def __init__(self, starting_state=12, path=None):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "        if path is None:\n",
    "            self.model = AutoModel.from_pretrained(\"bert-base-cased\").to(DEVICE)\n",
    "        else:\n",
    "            self.model = torch.load(path).to(DEVICE)\n",
    "        self.pooling = MeanPooling(starting_state)\n",
    "    \n",
    "    def prepare(self, params, samples):\n",
    "        pass \n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def batcher(self, params, batch):\n",
    "        tokenized_batch = self.tokenizer(\n",
    "            batch, truncation=True, padding=True, return_tensors=\"pt\", is_split_into_words=True\n",
    "        )\n",
    "        batch_device = batch_to_device(tokenized_batch, DEVICE)\n",
    "        out = self.model(\n",
    "            **batch_device, output_hidden_states=True\n",
    "        ).hidden_states[self.pooling.starting_state]\n",
    "        out_mean = self.pooling(out, batch_device[\"attention_mask\"])\n",
    "        return out_mean.cpu()\n",
    "\n",
    "params = {'task_path': PATH_TO_DATA, 'usepytorch': True, 'kfold': 10}\n",
    "params['classifier'] = {'nhid': 0, 'optim': 'adam', 'batch_size': 64, 'tenacity': 5, 'epoch_size': 4}\n",
    "\n",
    "bert = Bert(starting_state=12, path=\"../output/bert-base-cased/mean/12_to_13/model_2024_01_01_03_16.pkl\")\n",
    "se = senteval.engine.SE(params, bert.batcher, bert.prepare)\n",
    "\n",
    "transfer_tasks = [\n",
    "    'Length', \n",
    "    'WordContent', \n",
    "    'Depth', \n",
    "    'TopConstituents', \n",
    "    'BigramShift', \n",
    "    'Tense', \n",
    "    'SubjNumber', \n",
    "    'ObjNumber', \n",
    "    'OddManOut', \n",
    "    'CoordinationInversion'\n",
    "]\n",
    "\n",
    "results = se.eval(transfer_tasks)\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e631debd",
   "metadata": {},
   "source": [
    "# Finetuned state 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "421049f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"Length_acc\": 70.21,\n",
      "    \"WordContent_acc\": 57.61,\n",
      "    \"Depth_acc\": 31.33,\n",
      "    \"TopConstituents_acc\": 63.54,\n",
      "    \"BigramShift_acc\": 82.65,\n",
      "    \"Tense_acc\": 86.16,\n",
      "    \"SubjNumber_acc\": 80.76,\n",
      "    \"ObjNumber_acc\": 78.45,\n",
      "    \"OddManOut_acc\": 64.33,\n",
      "    \"CoordinationInversion_acc\": 67.01\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps({f\"{k}_acc\": v[\"acc\"] for k,v in results.items()}, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19f80df",
   "metadata": {},
   "source": [
    "# Pretrained (no finetuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59163653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Length': {'devacc': 81.7, 'acc': 82.87, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 59.79, 'acc': 59.3, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 37.26, 'acc': 37.9, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 74.85, 'acc': 74.71, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 89.0, 'acc': 88.58, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 90.42, 'acc': 88.54, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 85.61, 'acc': 84.56, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 80.98, 'acc': 82.41, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 66.17, 'acc': 65.69, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 69.43, 'acc': 68.98, 'ndev': 10002, 'ntest': 10002}}\n"
     ]
    }
   ],
   "source": [
    "params = {'task_path': PATH_TO_DATA, 'usepytorch': True, 'kfold': 10}\n",
    "params['classifier'] = {'nhid': 0, 'optim': 'adam', 'batch_size': 64, 'tenacity': 5, 'epoch_size': 4}\n",
    "\n",
    "bert2 = Bert(starting_state=12, path=None)\n",
    "se2 = senteval.engine.SE(params, bert2.batcher, bert2.prepare)\n",
    "\n",
    "results2 = se2.eval(transfer_tasks)\n",
    "print(results2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5dcef70e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"Length_acc\": 82.87,\n",
      "    \"WordContent_acc\": 59.3,\n",
      "    \"Depth_acc\": 37.9,\n",
      "    \"TopConstituents_acc\": 74.71,\n",
      "    \"BigramShift_acc\": 88.58,\n",
      "    \"Tense_acc\": 88.54,\n",
      "    \"SubjNumber_acc\": 84.56,\n",
      "    \"ObjNumber_acc\": 82.41,\n",
      "    \"OddManOut_acc\": 65.69,\n",
      "    \"CoordinationInversion_acc\": 68.98\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps({f\"{k}_acc\": v[\"acc\"] for k,v in results2.items()}, indent=4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
