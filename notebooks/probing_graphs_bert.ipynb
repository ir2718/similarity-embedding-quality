{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6a85fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import senteval\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "fs = os.listdir(\"..\")\n",
    "if not \"SentEval\" in fs:\n",
    "    %cd ./..\n",
    "    !git clone git@github.com:facebookresearch/SentEval.git\n",
    "    %cd notebooks\n",
    "    %mkdir data\n",
    "\n",
    "PATH_TO_DATA = \"../SentEval/data\"\n",
    "DEVICE = \"cuda:1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afee568e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_to_device(d, device):\n",
    "    return {k: v.to(device) for k, v in d.items()}\n",
    "    \n",
    "class MeanPooling(nn.Module):\n",
    "    def __init__(self, starting_state):\n",
    "        super().__init__()\n",
    "        self.starting_state = starting_state\n",
    "\n",
    "    def forward(self, x, attention_mask):\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(x.size()).float()\n",
    "        emb_sum = torch.sum(x * input_mask_expanded, dim=1)\n",
    "        sum_mask = torch.clamp(input_mask_expanded.sum(dim=1), min=1e-9) # denominator\n",
    "        emb_mean = emb_sum / sum_mask\n",
    "        return emb_mean\n",
    "\n",
    "class Electra:\n",
    "\n",
    "    def __init__(self, starting_state=12, path=None):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"google/electra-base-discriminator\")\n",
    "        if path is None:\n",
    "            self.model = AutoModel.from_pretrained(\"google/electra-base-discriminator\").to(DEVICE)\n",
    "        else:\n",
    "            self.model = torch.load(path).to(DEVICE)\n",
    "        self.pooling = MeanPooling(starting_state)\n",
    "    \n",
    "    def prepare(self, params, samples):\n",
    "        pass \n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def batcher(self, params, batch):\n",
    "        tokenized_batch = self.tokenizer(\n",
    "            batch, truncation=True, padding=True, return_tensors=\"pt\", is_split_into_words=True\n",
    "        )\n",
    "        batch_device = batch_to_device(tokenized_batch, DEVICE)\n",
    "        out = self.model(\n",
    "            **batch_device, output_hidden_states=True\n",
    "        ).hidden_states[self.pooling.starting_state]\n",
    "        out_mean = self.pooling(out, batch_device[\"attention_mask\"])\n",
    "        return out_mean.cpu()\n",
    "\n",
    "class Bert:\n",
    "\n",
    "    def __init__(self, starting_state=12, path=None):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "        if path is None:\n",
    "            self.model = AutoModel.from_pretrained(\"bert-base-cased\").to(DEVICE)\n",
    "        else:\n",
    "            self.model = torch.load(path).to(DEVICE)\n",
    "        self.pooling = MeanPooling(starting_state)\n",
    "    \n",
    "    def prepare(self, params, samples):\n",
    "        pass \n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def batcher(self, params, batch):\n",
    "        tokenized_batch = self.tokenizer(\n",
    "            batch, truncation=True, padding=True, return_tensors=\"pt\", is_split_into_words=True\n",
    "        )\n",
    "        batch_device = batch_to_device(tokenized_batch, DEVICE)\n",
    "        out = self.model(\n",
    "            **batch_device, output_hidden_states=True\n",
    "        ).hidden_states[self.pooling.starting_state]\n",
    "        out_mean = self.pooling(out, batch_device[\"attention_mask\"])\n",
    "        return out_mean.cpu()\n",
    "    \n",
    "transfer_tasks = [\n",
    "    'WordContent', \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "524cfad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                    | 0/12 [00:00<?, ?it/s]2024-01-10 17:39:56.840547: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-10 17:39:56.840572: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-10 17:39:56.840595: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-10 17:39:56.845783: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "100%|████████████████████████████████████████| 12/12 [1:37:42<00:00, 488.54s/it]\n"
     ]
    }
   ],
   "source": [
    "bert_values = []\n",
    "for i in tqdm(range(12)):\n",
    "    params = {'task_path': PATH_TO_DATA, 'usepytorch': True, 'kfold': 10}\n",
    "    params['classifier'] = {'nhid': 0, 'optim': 'adam', 'batch_size': 64, 'tenacity': 5, 'epoch_size': 4}\n",
    "\n",
    "    bert = Bert(starting_state=i)\n",
    "    se = senteval.engine.SE(params, bert.batcher, bert.prepare)\n",
    "\n",
    "    results = se.eval(transfer_tasks)\n",
    "    bert_values.append(results[\"WordContent\"][\"acc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4133e476",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[95.3,\n",
       " 93.09,\n",
       " 91.44,\n",
       " 88.81,\n",
       " 86.51,\n",
       " 83.36,\n",
       " 79.13,\n",
       " 75.38,\n",
       " 70.99,\n",
       " 66.94,\n",
       " 63.46,\n",
       " 61.56]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "bert_values"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
