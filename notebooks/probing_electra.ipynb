{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "521f6794",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import senteval\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import json\n",
    "import os\n",
    "\n",
    "fs = os.listdir(\"..\")\n",
    "if not \"SentEval\" in fs:\n",
    "    %cd ./..\n",
    "    !git clone git@github.com:facebookresearch/SentEval.git\n",
    "    %cd notebooks\n",
    "    %mkdir data\n",
    "\n",
    "PATH_TO_DATA = \"../SentEval/data\"\n",
    "DEVICE = \"cuda:0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e04a980",
   "metadata": {},
   "source": [
    "# ELECTRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b56d3905",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_to_device(d, device):\n",
    "    return {k: v.to(device) for k, v in d.items()}\n",
    "    \n",
    "class MeanPooling(nn.Module):\n",
    "    def __init__(self, starting_state):\n",
    "        super().__init__()\n",
    "        self.starting_state = starting_state\n",
    "\n",
    "    def forward(self, x, attention_mask):\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(x.size()).float()\n",
    "        emb_sum = torch.sum(x * input_mask_expanded, dim=1)\n",
    "        sum_mask = torch.clamp(input_mask_expanded.sum(dim=1), min=1e-9) # denominator\n",
    "        emb_mean = emb_sum / sum_mask\n",
    "        return emb_mean\n",
    "\n",
    "class Electra:\n",
    "\n",
    "    def __init__(self, starting_state=12, path=None):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"google/electra-base-discriminator\")\n",
    "        if path is None:\n",
    "            self.model = AutoModel.from_pretrained(\"google/electra-base-discriminator\").to(DEVICE)\n",
    "        else:\n",
    "            self.model = torch.load(path).to(DEVICE)\n",
    "        self.pooling = MeanPooling(starting_state)\n",
    "    \n",
    "    def prepare(self, params, samples):\n",
    "        pass \n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def batcher(self, params, batch):\n",
    "        tokenized_batch = self.tokenizer(\n",
    "            batch, truncation=True, padding=True, return_tensors=\"pt\", is_split_into_words=True\n",
    "        )\n",
    "        batch_device = batch_to_device(tokenized_batch, DEVICE)\n",
    "        out = self.model(\n",
    "            **batch_device, output_hidden_states=True\n",
    "        ).hidden_states[self.pooling.starting_state]\n",
    "        out_mean = self.pooling(out, batch_device[\"attention_mask\"])\n",
    "        return out_mean.cpu()\n",
    "\n",
    "transfer_tasks = [\n",
    "    'Length', \n",
    "    'WordContent', \n",
    "    'Depth', \n",
    "    'TopConstituents', \n",
    "    'BigramShift', \n",
    "    'Tense', \n",
    "    'SubjNumber', \n",
    "    'ObjNumber', \n",
    "    'OddManOut', \n",
    "    'CoordinationInversion'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9a0a8f9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-08 23:59:35.905490: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-08 23:59:35.905525: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-08 23:59:35.905553: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-08 23:59:35.911319: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Length': {'devacc': 84.24, 'acc': 82.69, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 29.11, 'acc': 28.59, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 36.23, 'acc': 36.04, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 70.32, 'acc': 69.59, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 92.9, 'acc': 92.41, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 86.67, 'acc': 85.26, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 81.76, 'acc': 81.41, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 79.34, 'acc': 80.09, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 72.79, 'acc': 72.28, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 74.98, 'acc': 74.19, 'ndev': 10002, 'ntest': 10002}}\n"
     ]
    }
   ],
   "source": [
    "params = {'task_path': PATH_TO_DATA, 'usepytorch': True, 'kfold': 10}\n",
    "params['classifier'] = {'nhid': 0, 'optim': 'adam', 'batch_size': 64, 'tenacity': 5, 'epoch_size': 4}\n",
    "\n",
    "electra = Electra(starting_state=12, path=\"../output/google-electra-base-discriminator/mean/12_to_13/model_2024_01_01_03_16.pkl\")\n",
    "se = senteval.engine.SE(params, electra.batcher, electra.prepare)\n",
    "\n",
    "results = se.eval(transfer_tasks)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dea2a25",
   "metadata": {},
   "source": [
    "# Finetuned state 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "db52b7a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"Length_acc\": 82.69,\n",
      "    \"WordContent_acc\": 28.59,\n",
      "    \"Depth_acc\": 36.04,\n",
      "    \"TopConstituents_acc\": 69.59,\n",
      "    \"BigramShift_acc\": 92.41,\n",
      "    \"Tense_acc\": 85.26,\n",
      "    \"SubjNumber_acc\": 81.41,\n",
      "    \"ObjNumber_acc\": 80.09,\n",
      "    \"OddManOut_acc\": 72.28,\n",
      "    \"CoordinationInversion_acc\": 74.19\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps({f\"{k}_acc\": v[\"acc\"] for k,v in results.items()}, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36db9cb4",
   "metadata": {},
   "source": [
    "# Pretrained (no finetuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4fe81b2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Length': {'devacc': 88.48, 'acc': 88.16, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 29.67, 'acc': 30.49, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 41.52, 'acc': 41.3, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 78.03, 'acc': 77.56, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 95.91, 'acc': 95.65, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 89.69, 'acc': 88.04, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 83.41, 'acc': 82.18, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 81.42, 'acc': 81.43, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 76.23, 'acc': 75.37, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 78.81, 'acc': 78.27, 'ndev': 10002, 'ntest': 10002}}\n"
     ]
    }
   ],
   "source": [
    "params = {'task_path': PATH_TO_DATA, 'usepytorch': True, 'kfold': 10}\n",
    "params['classifier'] = {'nhid': 0, 'optim': 'adam', 'batch_size': 64, 'tenacity': 5, 'epoch_size': 4}\n",
    "\n",
    "electra2 = Electra(starting_state=12, path=None)\n",
    "se2 = senteval.engine.SE(params, electra2.batcher, electra2.prepare)\n",
    "\n",
    "results2 = se2.eval(transfer_tasks)\n",
    "print(results2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b1c05a2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"Length_acc\": 88.16,\n",
      "    \"WordContent_acc\": 30.49,\n",
      "    \"Depth_acc\": 41.3,\n",
      "    \"TopConstituents_acc\": 77.56,\n",
      "    \"BigramShift_acc\": 95.65,\n",
      "    \"Tense_acc\": 88.04,\n",
      "    \"SubjNumber_acc\": 82.18,\n",
      "    \"ObjNumber_acc\": 81.43,\n",
      "    \"OddManOut_acc\": 75.37,\n",
      "    \"CoordinationInversion_acc\": 78.27\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps({f\"{k}_acc\": v[\"acc\"] for k,v in results2.items()}, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6091d7e",
   "metadata": {},
   "source": [
    "# Finetuned, state 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a9f8dd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Length': {'devacc': 87.29, 'acc': 86.74, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 40.91, 'acc': 40.22, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 36.87, 'acc': 36.68, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 70.9, 'acc': 70.79, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 87.93, 'acc': 86.94, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 88.96, 'acc': 86.88, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 84.71, 'acc': 83.9, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 80.99, 'acc': 82.11, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 66.64, 'acc': 66.2, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 69.22, 'acc': 67.9, 'ndev': 10002, 'ntest': 10002}}\n"
     ]
    }
   ],
   "source": [
    "params = {'task_path': PATH_TO_DATA, 'usepytorch': True, 'kfold': 10}\n",
    "params['classifier'] = {'nhid': 0, 'optim': 'adam', 'batch_size': 64, 'tenacity': 5, 'epoch_size': 4}\n",
    "\n",
    "electra3 = Electra(starting_state=9, path=\"./../output/google-electra-base-discriminator/mean/9_to_10/model_2024_01_09_16_53.pkl\")\n",
    "se3 = senteval.engine.SE(params, electra3.batcher, electra3.prepare)\n",
    "\n",
    "results3 = se3.eval(transfer_tasks)\n",
    "print(results3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e2eeece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"Length_acc\": 86.74,\n",
      "    \"WordContent_acc\": 40.22,\n",
      "    \"Depth_acc\": 36.68,\n",
      "    \"TopConstituents_acc\": 70.79,\n",
      "    \"BigramShift_acc\": 86.94,\n",
      "    \"Tense_acc\": 86.88,\n",
      "    \"SubjNumber_acc\": 83.9,\n",
      "    \"ObjNumber_acc\": 82.11,\n",
      "    \"OddManOut_acc\": 66.2,\n",
      "    \"CoordinationInversion_acc\": 67.9\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps({f\"{k}_acc\": v[\"acc\"] for k,v in results3.items()}, indent=4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
